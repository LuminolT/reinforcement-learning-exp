{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、 Policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from copy import copy\n",
    "\n",
    "\n",
    "def policy_eval(env, values, policies, upper_bound):\n",
    "    iteration=0\n",
    "    delta=upper_bound\n",
    "    while delta>=upper_bound:\n",
    "        delta=0\n",
    "        tempv=np.zeros(env.state_space)\n",
    "        for s in env.states:\n",
    "            probs=policies.retrieve(s)\n",
    "            #print('probs={}'.format(probs))\n",
    "            v=0\n",
    "            for action in env.actions:\n",
    "                env.set_state(s)\n",
    "                _,done,reward,state=env.step(act=action)\n",
    "                v+=probs[action]*(reward+env.gamma*values.get(state)[0])\n",
    "            tempv[s]=v\n",
    "            delta=max(delta,abs(v-values.get(s)))\n",
    "        for s in env.states: \n",
    "            values.update(s=s,value=tempv[s])\n",
    "        iteration += 1\n",
    "        # print('\\r> iteration: {} delta: {}'.format(iteration, delta), flush=True, end=\"\")\n",
    "        print('\\r> delta={},iteration={}'.format(delta,iteration), flush=True, end=\"\")\n",
    "    \n",
    "    ## 遍历棋盘中的每一个点，更新Value Function\n",
    "    ##                         Bellman Equation\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improve(env, values, policies):\n",
    "    ## 遍历棋盘中的每一个点，更新Policy := action指向max(value(s'))\n",
    "    stable=1\n",
    "    for s in env.states:\n",
    "        maximun=0.\n",
    "        point=0\n",
    "        r=0\n",
    "        for i,action in enumerate(env.actions):\n",
    "            env.set_state(s)\n",
    "            _,done,reward,state=env.step(act=action)\n",
    "            val=values.get(state[0])\n",
    "            #print('val={}'.format(val))\n",
    "            #print('s={},i={},val={}'.format(s,i,val))\n",
    "            if val>maximun:\n",
    "                maximun=val\n",
    "                #print('max={},equal={}'.format(maximun,maximun==val))\n",
    "                point=i\n",
    "                r=reward[0]\n",
    "            elif val==maximun:\n",
    "                if reward[0]>r:\n",
    "                    maximun=val\n",
    "                    point=i\n",
    "                    r=reward[0]\n",
    "            #print('i={},val={},maximun={},point={}'.format(i,val,maximun,point))\n",
    "        #print('point={}'.format(point))\n",
    "        probs=policies.retrieve(s)\n",
    "        new_policy = [0.] * env.action_space\n",
    "        new_policy[point] = 1.\n",
    "        policies.update(s, new_policy)\n",
    "        if probs==new_policy:\n",
    "            stable=1\n",
    "        else:\n",
    "            stable=0\n",
    "    return stable\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iter(env, values, upper_bound):\n",
    "    ## 遍历棋盘中的每一个点，向max(value(s'))更新，并且更新更新Policy := action指向max(value(s'))\n",
    "    ##                       这个意思是Bellman Optimal Equation\n",
    "    delta=upper_bound+1\n",
    "    iteration=0\n",
    "    while delta>=upper_bound:\n",
    "        delta=0\n",
    "        tempv=np.zeros(env.state_space)\n",
    "        for s in env.states:\n",
    "            maximun=0\n",
    "            probs=policies.retrieve(s)\n",
    "            for i,action in enumerate(env.actions):\n",
    "                env.set_state(s)\n",
    "                _,done,reward,state=env.step(act=action)\n",
    "                v=reward[0]+env.gamma*values.get(state)[0]\n",
    "                #print('v={}'.format(v))\n",
    "                maximun=max(maximun,v)\n",
    "            tempv[s]=maximun\n",
    "            #print('maximun={}'.format(maximun))\n",
    "            delta=max(delta,abs(maximun-values.get(s)))\n",
    "        for s in env.states: \n",
    "            values.update(s=s,value=tempv[s])\n",
    "        iteration += 1\n",
    "    print('\\r> delta={},iteration={}'.format(delta,iteration), flush=True, end=\"\")\n",
    "        \n",
    "    \n",
    "                    \n",
    "                    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env:\n",
    "    def __init__(self):\n",
    "        self._states = set()\n",
    "        self._state = None\n",
    "        self._actions = []\n",
    "        self._gamma = None\n",
    "        \n",
    "    @property\n",
    "    def states(self):\n",
    "        return self._states\n",
    "    \n",
    "    @property\n",
    "    def state_space(self):\n",
    "        return self._state_shape\n",
    "    \n",
    "    @property\n",
    "    def actions(self):\n",
    "        return self._actions\n",
    "    \n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return len(self._actions)\n",
    "    \n",
    "    @property\n",
    "    def gamma(self):\n",
    "        return self._gamma\n",
    "    \n",
    "    def _world_init(self):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def reset(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def step(self, state, action):\n",
    "        \"\"\"Return distribution and next states\"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def set_state(self, state):\n",
    "        self._state = state\n",
    "\n",
    "\n",
    "class MatrixEnv(Env):\n",
    "    def __init__(self, height=4, width=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self._action_space = 4\n",
    "        self._actions = list(range(4))\n",
    "        \n",
    "        # self._state_shape = (2,)\n",
    "        self._state_shape = (height, width)\n",
    "        self._states = [(i, j) for i in range(height) for j in range(width)]\n",
    "        \n",
    "        self._gamma = 0.9\n",
    "        self._height = height\n",
    "        self._width = width\n",
    "\n",
    "        self._world_init()\n",
    "        \n",
    "    @property\n",
    "    def state(self):\n",
    "        return self._state\n",
    "    \n",
    "    @property\n",
    "    def gamma(self):\n",
    "        return self._gamma\n",
    "    \n",
    "    def set_gamma(self, value):\n",
    "        self._gamma = value\n",
    "        \n",
    "    def reset(self):\n",
    "        self._state = self._start_point\n",
    "        \n",
    "    def _world_init(self):\n",
    "        # start_point\n",
    "        self._start_point = (0, 0)\n",
    "        self._end_point = (self._height - 1, self._width - 1)\n",
    "        \n",
    "    def _state_switch(self, act):\n",
    "        # 0: h - 1, 1: w + 1, 2: h + 1, 3: w - 1\n",
    "        if act == 0:  # up\n",
    "            self._state = (max(0, self._state[0] -1), self._state[1])\n",
    "        elif act == 1:  # right\n",
    "            self._state = (self._state[0], min(self._width - 1, self._state[1] + 1))\n",
    "        elif act == 2:  # down\n",
    "            self._state = (min(self._height - 1, self._state[0] + 1), self._state[1])\n",
    "        elif act == 3:  # left\n",
    "            self._state = (self._state[0], max(0, self._state[1] - 1))\n",
    "\n",
    "    def step(self, act):\n",
    "\n",
    "        ## Wrong here\n",
    "        assert 0 <= act <= 3\n",
    "        \n",
    "        done = False\n",
    "        reward = 0.\n",
    "\n",
    "        self._state_switch(act)\n",
    "        \n",
    "        if self._state == self._end_point:\n",
    "            reward = 1.\n",
    "            done = True\n",
    "\n",
    "        return None, done, [reward], [self._state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueTable:\n",
    "    def __init__(self, env):\n",
    "        self._values = np.zeros(env.state_space)\n",
    "        \n",
    "    def update(self, s, value):\n",
    "        self._values[s] = value\n",
    "        \n",
    "    def get(self, state):\n",
    "        if type(state) == list:\n",
    "            # loop get\n",
    "            res = [self._values[s] for s in state]\n",
    "            return res\n",
    "        elif type(state) == tuple:\n",
    "            # return directly\n",
    "            return self._values[state]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "\n",
    "Pi = namedtuple('Pi', 'act, prob')\n",
    "\n",
    "\n",
    "class Policies:\n",
    "    def __init__(self, env: Env):\n",
    "        self._actions = env.actions\n",
    "        self._default_policy = [1 / env.action_space] * env.action_space\n",
    "        self._policies = dict.fromkeys(env.states, Pi(self._actions, self._default_policy))\n",
    "    \n",
    "    def sample(self, state):\n",
    "        if self._policies.get(state, None) is None:\n",
    "            self._policies[state] = Pi(self._actions, self._default_policy)\n",
    "\n",
    "        policy = self._policies[state]\n",
    "        return np.random.choice(policy.act, p=policy.prob)\n",
    "    \n",
    "    def retrieve(self, state):\n",
    "        return self._policies[state].prob\n",
    "    \n",
    "    def update(self, state, policy):\n",
    "        self._policies[state] = self._policies[state]._replace(prob=policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> delta=[9.35741733e-05],iteration=87\n",
      "[time consumpution]: 0.46455955505371094 s\n",
      "start->[(0, 1)]->[(0, 2)]->[(0, 3)]->[(0, 4)]->[(0, 5)]->[(0, 6)]->[(0, 7)]->[(1, 7)]->[(2, 7)]->[(3, 7)]->[(4, 7)]->[(5, 7)]->[(6, 7)]->end\n",
      "\n",
      "Evaluation: [reward] 1.0 [step] 14\n"
     ]
    }
   ],
   "source": [
    "# import time\n",
    "\n",
    "# env = MatrixEnv(width=8, height=8)  # TODO(ming): try different word size\n",
    "# policies = Policies(env)\n",
    "# values = ValueTable(env)\n",
    "# upper_bound = 1e-4\n",
    "\n",
    "# stable = False\n",
    "\n",
    "# policy_eval(env, values, policies, upper_bound)\n",
    "\n",
    "# start = time.time()\n",
    "# # while not stable:\n",
    "# #     policy_eval(env, values, policies, upper_bound)\n",
    "# #     break\n",
    "# #     stable = policy_improve(env, values, policies)\n",
    "# # end = time.time()\n",
    "# # print('\\n[time consumpution]: {} s'.format(end - start))\n",
    "\n",
    "# if 1 == 1:\n",
    "\n",
    "#     done = False\n",
    "#     rewards = 0\n",
    "#     env.reset()\n",
    "#     step = 0\n",
    "\n",
    "#     while not done:\n",
    "#         act_index = policies.sample(env.state)\n",
    "#         _, done, reward, next_state = env.step(env.actions[act_index])\n",
    "#         rewards += sum(reward)\n",
    "#         step += 1\n",
    "\n",
    "#     print('Evaluation: [reward] {} [step] {}'.format(rewards, step))\n",
    "import time\n",
    "\n",
    "env = MatrixEnv(width=8, height=8)  # TODO(ming): try different word size\n",
    "policies = Policies(env)\n",
    "values = ValueTable(env)\n",
    "upper_bound = 1e-4\n",
    "\n",
    "stable = False\n",
    "\n",
    "start = time.time()\n",
    "while not stable:\n",
    "    policy_eval(env, values, policies, upper_bound)\n",
    "    stable = policy_improve(env, values, policies)\n",
    "end = time.time()\n",
    "\n",
    "print('\\n[time consumpution]: {} s'.format(end - start))\n",
    "\n",
    "done = False\n",
    "rewards = 0\n",
    "env.reset()\n",
    "step = 0\n",
    "print('start->',end=\"\")\n",
    "while not done:\n",
    "    act_index = policies.sample(env.state)\n",
    "    _, done, reward, next_state = env.step(env.actions[act_index])\n",
    "    rewards += sum(reward)\n",
    "    print('{}->'.format(next_state), end='') if not done else print('end')\n",
    "    step += 1\n",
    "# print('\\b'*8 + 'end')\n",
    "print()\n",
    "print('Evaluation: [reward] {} [step] {}'.format(rewards, step))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> delta=9.404610870067387e-05,iteration=89\n",
      "[time consumption] 0.08078312873840332s\n",
      "Evaluation: [reward] 1.0 [step] 14\n"
     ]
    }
   ],
   "source": [
    "env = MatrixEnv(width=8, height=8)  # try different word size\n",
    "policies = Policies(env)\n",
    "values = ValueTable(env)\n",
    "upper_bound = 1e-4\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "value_iter(env, values, upper_bound)\n",
    "_ = policy_improve(env, values, policies)\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print('\\n[time consumption] {}s'.format(end - start))\n",
    "# print(\"===== Render =====\")\n",
    "env.reset()\n",
    "done = False\n",
    "rewards = 0\n",
    "step = 0\n",
    "while not done:\n",
    "    act_index = policies.sample(env.state)\n",
    "    _, done, reward, next_state = env.step(env.actions[act_index])\n",
    "    rewards += sum(reward)\n",
    "    #print('{}->'.format(next_state), end='') if not done else print('end')\n",
    "    step += 1\n",
    "\n",
    "print('Evaluation: [reward] {} [step] {}'.format(rewards, step))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
